# # An Argo Training Pipeline with Runhouse

# This example demonstrates how to use Argo to orchestrate the training and inference for a recommender model
# with the training function dispatched to a GPU-enabled AWS cloud instance with Ray enabled to actually do the work.
#
# We use the very popular MovieLens dataset, which is a dataset of movie ratings. We will train a deep learning model
# to predict the ratings of movies based on user and movie features.
#
# ## Setup credentials and dependencies
# For this example, we will need AWS cloud credentials and a Runhouse API key. We name this secret `my-secret` for simplicity, and use it in the pipeline.
# kubectl create secret generic my-secret \
#   --from-literal=AWS_ACCESS_KEY_ID=<your-access-key-id> \
#   --from-literal=AWS_SECRET_ACCESS_KEY=<your-secret-access-key> \
#   --from-literal=RUNHOUSE_API_KEY=<your-runhouse-api-key>
#
# We'll be launching elastic compute from AWS from within the first step. You can either reuse the same compute across steps, or launch fresh compute per step.
# Reusing the cluster provides a lot of benefits, such as statefulness and minimized I/O overhead, while launching fresh compute lets you optimize for cost, and make use of queuing if that is setup in Runhouse.
#
# ## Setting up the Argo pipeline
# The code actually executed by the pipeline is extremely lean, with each step being setup and dispatch for the Runhouse functions.
# The main for local script-style execution is nearly identical to the code put into the pipeline. This enables complete
# reproducibility across research, production, and debugging.

apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: ray-training-pipeline-
spec:
  entrypoint: ray-training-pipeline
  templates:
  - name: ray-training-pipeline
    steps:
    - - name: preprocess-data
        template: preprocess-data-task
    - - name: train-model
        template: train-model-task
    - - name: inference-data
        template: inference-task

  - name: preprocess-data-task
    script:
      image: pypypypy/my-pipeline-image:latest
      command: [python]
      env:
      - name: RUNHOUSE_API_KEY
        valueFrom:
          secretKeyRef:
            name: my-secret
            key: RUNHOUSE_API_KEY
      source: |
        # We show the code here for simpler illustration of the workflow, but Runhouse is agnostic to how the steps is run.
        import os, sys, runhouse as rh

        # First we configure the environment to setup Runhouse and AWS credentials.
        rh.login(token=os.getenv("RUNHOUSE_API_KEY"), interactive=False)
        aws_secret = rh.secret("aws")
        aws_secret._write_to_file('~/.aws/credentials',aws_secret.values)

        # Now we can bring up the cluster with Runhouse.
        num_nodes = 2

        img = rh.Image('ray-data').install_packages(
            [
                "ray[data]",
                "pandas",
                "scikit-learn",
                "torch",
                "awscli",
            ]
        ).sync_secrets([
            "aws"
            ]
        )

        num_nodes = 2

        cluster = rh.compute(
            name="rh-ray-preprocessing",
            num_cpus="4",
            memory = "15+",
            provider="aws",
            region="us-east-1",
            num_nodes=num_nodes,
            autostop_minutes=120,
            image = img,
        ).up_if_not()

        # Set some variables (maybe we would not hard code this in a proper production)
        s3_raw = 's3://rh-demo-external/dlrm-training-example/raw_data'
        local_path = '~/dlrm'
        s3_preprocessed = 's3://rh-demo-external/dlrm-training-example/preprocessed_data'

        # The underlying functions that we run are baked into the container image, but run identically to functions we run locally
        # Here, we import the preprocessing function before sending it off to the remote cluster we launched in this step
        sys.path.append(os.path.expanduser("~/code/"))
        from dlrm_data_preproc import preprocess_data

        remote_preprocess = rh.function(preprocess_data).to(cluster, name = 'preprocess_data').distribute('ray')
        print('sent function to cluster')
        remote_preprocess(s3_read_path = s3_raw
                          , s3_write_path = s3_preprocessed)

        cluster.teardown()
  # This step represents a step to access and lightly preprocess the dataset. The MNIST example is trivial, but it is worth calling out that we are doing this preprocessing on the same compute we will use later to do the training and we do not need to re-access the data or re-download it.
  - name: train-model-task
    script:
      image: pypypypy/my-pipeline-image:latest
      command: [python]
      env:
      - name: RUNHOUSE_API_KEY
        valueFrom:
          secretKeyRef:
            name: my-secret
            key: RUNHOUSE_API_KEY
      source: |
        import sys, os, runhouse as rh
        rh.login(token=os.getenv("RUNHOUSE_API_KEY"), interactive=False)
        aws_secret = rh.secret("aws")
        aws_secret._write_to_file('~/.aws/credentials',aws_secret.values)

        sys.path.append(os.path.expanduser("~/code/"))
        from dlrm_training import ray_trainer

        # Create a cluster of 3 GPUs
        gpus_per_node = 1
        num_nodes = 3

        img = rh.Image('ray-torch').install_packages(["torch==2.5.1", "datasets", "boto3", "awscli", "ray[data,train]"]).sync_secrets(["aws"])

        gpu_cluster = (
            rh.compute(
                name=f"rh-{num_nodes}x{gpus_per_node}GPU",
                gpus=f"A10G:{gpus_per_node}",
                num_nodes=num_nodes,
                provider="aws",
                image = img,
            )
            .up_if_not()
        )

        epochs = 15
        train_data_path = "s3://rh-demo-external/dlrm-training-example/preprocessed_data/train/"
        val_data_path ="s3://rh-demo-external/dlrm-training-example/preprocessed_data/eval/"
        working_s3_bucket = "rh-demo-external"
        working_s3_path = "dlrm-training-example/"

        remote_trainer = rh.function(ray_trainer).to(gpu_cluster, name = "ray_trainer").distribute('ray')

        remote_trainer(
            num_nodes,
            gpus_per_node,
            s3_bucket=working_s3_bucket,
            s3_path=working_s3_path,
            train_data_path=train_data_path,
            val_data_path=val_data_path,
            embedding_dim=64,
            lr=0.001,
            weight_decay=0.0001,
            step_size=5,
            gamma=0.5,
            epochs=epochs,
            save_every_epochs=5)

        cluster.teardown()

  # Now we run the training. In this step, we dispatch the training to the remote cluster. The model is trained on the remote cluster, and the model checkpoints are saved to an S3 bucket.
  - name: inference-task
    script:
      image: pypypypy/my-pipeline-image:latest
      command: [python]
      env:
      - name: RUNHOUSE_API_KEY
        valueFrom:
          secretKeyRef:
            name: my-secret
            key: RUNHOUSE_API_KEY
      source: |
        import sys, os, runhouse as rh
        rh.login(token=os.getenv("RUNHOUSE_API_KEY"), interactive=False)
        aws_secret = rh.secret("aws")
        aws_secret._write_to_file('~/.aws/credentials',aws_secret.values)

        sys.path.append(os.path.expanduser("~/code/"))

        gpus_per_node = 1
        num_nodes = 2

        img = rh.Image("ray-data").install_packages(["torch==2.5.1", "datasets", "boto3", "awscli", "ray[data,train]"]).sync_secrets(["aws"])

        gpu_cluster = (
            rh.compute(
                name=f"rh-{num_nodes}x{gpus_per_node}GPU",
                gpus=f"A10G:{gpus_per_node}",
                num_nodes=num_nodes,
                provider="aws",
                autostop_minutes=45,
                image = img,
            )
            .up_if_not()
            .save()
        )

        from dlrm_inference import inference_dlrm
        remote_inference = rh.function(inference_dlrm).to(gpu_cluster, name = "inference_dlrm").distribute('ray')

        remote_inference(num_gpus = gpus_per_node,
                        num_nodes = num_nodes,
                        model_s3_bucket="rh-demo-external",
                        model_s3_key="dlrm-training-example/checkpoints/dlrm_model.pth",
                        dataset_s3_path = "s3://rh-demo-external/dlrm-training-example/preprocessed_data/test/",
                        write_s3_path = "s3://rh-demo-external/dlrm-training-example/predictions/")

        gpu_cluster.teardown()
