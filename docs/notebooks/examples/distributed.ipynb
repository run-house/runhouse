{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yD2zHq2PjZ9S"
      },
      "source": [
        "# MultiGPU Distributed Launching with Runhouse and HuggingFace Accelerate\n",
        "\n",
        "This tutorial demonstrates how to use Runhouse with HuggingFace accelerate to launch distributed code on **your own remote hardware**. We also show how one can reproducibly perform hardware dependency autosetup, to ensure that your code runs smoothly every time.\n",
        "\n",
        "You can run this on your own cluster, or through a standard cloud account (AWS, GCP, Azure, LambdaLabs). If you do not have any compute or cloud accounts set up, we recommend creating a [LambdaLabs](https://cloud.lambdalabs.com/) account for the easiest setup path."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0ES9dq0Nklx0"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMv0FVa8kjTu"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate\n",
        "!pip install runhouse[sky]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vdq9J2HPk3MT",
        "outputId": "1938837b-80fa-421a-b071-23dfac400a94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO | 2023-03-20 17:56:13,023 | No auth token provided, so not using RNS API to save and load configs\n",
            "INFO | 2023-03-20 17:56:14,334 | NumExpr defaulting to 2 threads.\n"
          ]
        }
      ],
      "source": [
        "import runhouse as rh"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QLdW-sOykDRd"
      },
      "source": [
        "## Setting up the Cluster"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kIES_DUKSm46"
      },
      "source": [
        "### On-Demand Cluster (AWS, Azure, GCP, or LambdaLabs)\n",
        "\n",
        "For instructions on setting up cloud access for on-demand clusters, please refer to\n",
        "[Cluster Setup](https://www.run.house/docs/tutorials/quick_start#cluster-setup)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "ba7410d54dba475c9806a707b9be6d11",
            "20e3d132b7ef4979916c341aea03dd2f"
          ]
        },
        "id": "YvrIvtM0kf0Z",
        "outputId": "ab8f0aea-9a7f-4ae9-f02e-2656778d86af"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba7410d54dba475c9806a707b9be6d11",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# single V100 GPU\n",
        "# gpu = rh.cluster(name=\"rh-v100\", instance_type=\"V100:1\").up_if_not()\n",
        "\n",
        "# multigpu: 4 V100s\n",
        "gpu = rh.cluster(name=\"rh-4-v100\", instance_type=\"V100:4\").up_if_not()\n",
        "\n",
        "# Set GPU to autostop after 60 min of inactivity (default is 30 min)\n",
        "gpu.keep_warm(60)  # or -1 to keep up indefinitely"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UVGcCFR2So5M"
      },
      "source": [
        "### On-Premise Cluster"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xlCfq34K0HPW"
      },
      "source": [
        "For an on-prem cluster, you can instantaite it as follows, filling in the IP address, ssh user and private key path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBqRkQiHSoaU"
      },
      "outputs": [],
      "source": [
        "# For an existing cluster\n",
        "# gpu = rh.cluster(ips=['<ip of the cluster>'], \n",
        "#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},\n",
        "#                  name='rh-cluster')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jibiSAO3kHmq"
      },
      "source": [
        "## Setting up Functions on Remote Hardware"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N3UYyt19ZAX6"
      },
      "source": [
        "### Training Function\n",
        "For simplicity, let's use the [training_function](https://github.com/huggingface/accelerate/blob/main/examples/nlp_example.py#L114) from [accelerate/examples/nlp_example.py](https://github.com/huggingface/accelerate/blob/v0.15.0/examples/nlp_example.py) to demonstrate how to run this function remotely.\n",
        "\n",
        "In this case, because the function is available on GitHub, we can pass in a string pointing to the GitHub function.\n",
        "\n",
        "For local functions, for instance if we had `nlp_example.py` in our directory, we can also simply import the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRWEivhdkC4f"
      },
      "outputs": [],
      "source": [
        "# if nlp_example.py is in local directory\n",
        "# from nlp_example import training_function\n",
        "\n",
        "# if function is available on GitHub, use it's string representation\n",
        "training_function = \"https://github.com/huggingface/accelerate/blob/v0.15.0/examples/nlp_example.py:training_function\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-DCrBakxkzIj"
      },
      "source": [
        "Next, define the dependencies necessary to run the imported training function using accelerate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMd3B3Askw9w"
      },
      "outputs": [],
      "source": [
        "reqs = ['pip:./accelerate', 'transformers', 'datasets', 'evaluate','tqdm', 'scipy', 'scikit-learn', 'tensorboard',\n",
        "        'torch --upgrade --extra-index-url https://download.pytorch.org/whl/cu117']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sSGnAfWSlt7j"
      },
      "source": [
        "Now, we can put together the above components (gpu cluster, training function, and dependencies) to create our train function on remote hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU9lvTWiksjA",
        "outputId": "737c6382-45ce-40da-b66c-8327070c7bb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO | 2023-03-20 21:01:46,942 | Setting up Function on cluster.\n",
            "INFO | 2023-03-20 21:01:46,951 | Installing packages on cluster rh-v100: ['GitPackage: https://github.com/huggingface/accelerate.git@v0.15.0', 'pip:./accelerate', 'transformers', 'datasets', 'evaluate', 'tqdm', 'scipy', 'scikit-learn', 'tensorboard', 'torch --upgrade --extra-index-url https://download.pytorch.org/whl/cu117']\n",
            "INFO | 2023-03-20 21:02:02,988 | Function setup complete.\n"
          ]
        }
      ],
      "source": [
        "train_function_gpu = rh.function(\n",
        "                          fn=training_function,\n",
        "                          system=gpu,\n",
        "                          reqs=reqs,\n",
        "                      )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tH93EYV-09u-"
      },
      "source": [
        "`train_function_gpu` is a callable that can be used just like the original `training_function` function in the NLP example, except that it runs the function on the specified cluster/system instead."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UIgACtVHXPL9"
      },
      "source": [
        "## Launch Helper Function\n",
        "\n",
        "Here we define a helper function for launching accelerate training, and then send the function to run on our GPU as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRrn41i3W_0M"
      },
      "outputs": [],
      "source": [
        "def launch_training(training_function, *args):\n",
        "    from accelerate.utils import PrepareForLaunch, patch_environment\n",
        "    import torch\n",
        "\n",
        "    num_processes = torch.cuda.device_count()\n",
        "    print(f'Device count: {num_processes}')\n",
        "    with patch_environment(world_size=num_processes, master_addr=\"127.0.0.1\", master_port=\"29500\",\n",
        "                           mixed_precision=args[1].mixed_precision):\n",
        "        launcher = PrepareForLaunch(training_function, distributed_type=\"MULTI_GPU\")\n",
        "        torch.multiprocessing.start_processes(launcher, args=args, nprocs=num_processes, start_method=\"spawn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4PxOzfRjXkK",
        "outputId": "ad36060f-cc86-4f3e-a209-2b97d23052b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO | 2023-03-20 19:56:15,257 | Writing out function function to /content/launch_training_fn.py as functions serialized in notebooks are brittle. Please make sure the function does not rely on any local variables, including imports (which should be moved inside the function body).\n",
            "INFO | 2023-03-20 19:56:15,262 | Setting up Function on cluster.\n",
            "INFO | 2023-03-20 19:56:15,265 | Copying local package content to cluster <rh-v100>\n",
            "INFO | 2023-03-20 19:56:20,623 | Installing packages on cluster rh-v100: ['./']\n",
            "INFO | 2023-03-20 19:56:20,753 | Function setup complete.\n"
          ]
        }
      ],
      "source": [
        "launch_training_gpu = rh.function(fn=launch_training).to(gpu)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "00HdkU43Y54a"
      },
      "source": [
        "## Launch Distributed Training\n",
        "Now, we're ready to launch distributed training on our self-hosted hardware! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvfaG59ZY5Tr"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "\n",
        "# define basic train args and hyperparams\n",
        "train_args = argparse.Namespace(cpu=False, mixed_precision='fp16')\n",
        "hps = {\"lr\": 2e-5, \"num_epochs\": 3, \"seed\": 42, \"batch_size\": 16}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDvIEqUsQlo7",
        "outputId": "e60eaa9c-401a-4047-fd75-c79c94c493b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO | 2023-03-20 20:11:45,415 | Running launch_training via gRPC\n",
            "INFO | 2023-03-20 20:11:45,718 | Time to send message: 0.3 seconds\n",
            "INFO | 2023-03-20 20:11:45,720 | Submitted remote call to cluster. Result or logs can be retrieved\n",
            " with run_key \"launch_training_20230320_201145\", e.g. \n",
            "`rh.cluster(name=\"~/rh-v100\").get(\"launch_training_20230320_201145\", stream_logs=True)` in python \n",
            "`runhouse logs \"rh-v100\" launch_training_20230320_201145` from the command line.\n",
            " or cancelled with \n",
            "`rh.cluster(name=\"~/rh-v100\").cancel(\"launch_training_20230320_201145\")` in python or \n",
            "`runhouse cancel \"rh-v100\" launch_training_20230320_201145` from the command line.\n",
            ":task_name:launch_training\n",
            ":task_name:launch_training\n",
            "INFO | 2023-03-20 20:11:46,328 | Loading config from local file /home/ubuntu/runhouse/runhouse/builtins/config.json\n",
            "INFO | 2023-03-20 20:11:46,328 | No auth token provided, so not using RNS API to save and load configs\n",
            "Device count: 1\n",
            "INFO | 2023-03-20 20:11:49,486 | Loading config from local file /home/ubuntu/runhouse/runhouse/builtins/config.json\n",
            "INFO | 2023-03-20 20:11:49,486 | No auth token provided, so not using RNS API to save and load configs\n",
            "INFO | 2023-03-20 20:11:49,844 | Appending /home/ubuntu/accelerate/examples to sys.path\n",
            "INFO | 2023-03-20 20:11:49,844 | Importing module nlp_example\n",
            "\n",
            "Downloading builder script:   0%|          | 0.00/5.75k [00:00<?, ?B/s]\n",
            "Downloading builder script: 100%|██████████| 5.75k/5.75k [00:00<00:00, 8.19MB/s]\n",
            "\n",
            "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]\n",
            "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 65.9kB/s]\n",
            "\n",
            "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]\n",
            "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 1.22MB/s]\n",
            "\n",
            "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 1.05MB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 1.05MB/s]\n",
            "\n",
            "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]\n",
            "Downloading (…)/main/tokenizer.json: 100%|██████████| 436k/436k [00:00<00:00, 1.61MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100%|██████████| 436k/436k [00:00<00:00, 1.60MB/s]\n",
            "\n",
            "Downloading builder script:   0%|          | 0.00/28.8k [00:00<?, ?B/s]\n",
            "Downloading builder script: 100%|██████████| 28.8k/28.8k [00:00<00:00, 380kB/s]\n",
            "\n",
            "Downloading metadata:   0%|          | 0.00/28.7k [00:00<?, ?B/s]\n",
            "Downloading metadata: 100%|██████████| 28.7k/28.7k [00:00<00:00, 422kB/s]\n",
            "\n",
            "Downloading readme:   0%|          | 0.00/27.9k [00:00<?, ?B/s]\n",
            "Downloading readme: 100%|██████████| 27.9k/27.9k [00:00<00:00, 412kB/s]\n",
            "\n",
            "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]Downloading and preparing dataset glue/mrpc to /home/ubuntu/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "\n",
            "\n",
            "Downloading data: 0.00B [00:00, ?B/s]\u001b[A\n",
            "Downloading data: 6.22kB [00:00, 11.1MB/s]\n",
            "\n",
            "Downloading data files:  33%|███▎      | 1/3 [00:00<00:00,  4.26it/s]\n",
            "\n",
            "Downloading data: 0.00B [00:00, ?B/s]\u001b[A\n",
            "Downloading data: 1.05MB [00:00, 55.0MB/s]\n",
            "\n",
            "Downloading data files:  67%|██████▋   | 2/3 [00:00<00:00,  5.30it/s]\n",
            "\n",
            "Downloading data: 0.00B [00:00, ?B/s]\u001b[A\n",
            "Downloading data: 441kB [00:00, 44.3MB/s]\n",
            "\n",
            "Downloading data files: 100%|██████████| 3/3 [00:00<00:00,  5.87it/s]\n",
            "Downloading data files: 100%|██████████| 3/3 [00:00<00:00,  5.56it/s]\n",
            "\n",
            "Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]\n",
            "Generating train split:  79%|███████▉  | 2898/3668 [00:00<00:00, 28934.98 examples/s]\n",
            "                                                                                     \n",
            "\n",
            "Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]\n",
            "                                                                           \n",
            "\n",
            "Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]\n",
            "                                                                      \n",
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 1296.81it/s]\n",
            "\n",
            "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 3668/3668 [00:00<00:00, 33587.18 examples/s]\n",
            "                                                                  \n",
            "\n",
            "Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n",
            "                                                   \n",
            "\n",
            "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n",
            "                                                    \n",
            "Dataset glue downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "\n",
            "Downloading pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]\n",
            "Downloading pytorch_model.bin:   2%|▏         | 10.5M/436M [00:00<00:04, 95.8MB/s]\n",
            "Downloading pytorch_model.bin:   5%|▍         | 21.0M/436M [00:00<00:04, 97.1MB/s]\n",
            "Downloading pytorch_model.bin:   7%|▋         | 31.5M/436M [00:00<00:04, 93.2MB/s]\n",
            "Downloading pytorch_model.bin:  10%|▉         | 41.9M/436M [00:00<00:04, 91.3MB/s]\n",
            "Downloading pytorch_model.bin:  12%|█▏        | 52.4M/436M [00:00<00:04, 92.6MB/s]\n",
            "Downloading pytorch_model.bin:  14%|█▍        | 62.9M/436M [00:00<00:04, 86.0MB/s]\n",
            "Downloading pytorch_model.bin:  17%|█▋        | 73.4M/436M [00:00<00:04, 89.9MB/s]\n",
            "Downloading pytorch_model.bin:  19%|█▉        | 83.9M/436M [00:00<00:03, 90.2MB/s]\n",
            "Downloading pytorch_model.bin:  22%|██▏       | 94.4M/436M [00:01<00:03, 91.5MB/s]\n",
            "Downloading pytorch_model.bin:  24%|██▍       | 105M/436M [00:01<00:03, 93.3MB/s] \n",
            "Downloading pytorch_model.bin:  26%|██▋       | 115M/436M [00:01<00:03, 86.5MB/s]\n",
            "Downloading pytorch_model.bin:  29%|██▉       | 126M/436M [00:01<00:03, 86.9MB/s]\n",
            "Downloading pytorch_model.bin:  31%|███▏      | 136M/436M [00:01<00:03, 87.2MB/s]\n",
            "Downloading pytorch_model.bin:  34%|███▎      | 147M/436M [00:01<00:03, 88.6MB/s]\n",
            "Downloading pytorch_model.bin:  36%|███▌      | 157M/436M [00:01<00:03, 90.7MB/s]\n",
            "Downloading pytorch_model.bin:  38%|███▊      | 168M/436M [00:01<00:02, 90.4MB/s]\n",
            "Downloading pytorch_model.bin:  41%|████      | 178M/436M [00:02<00:03, 82.5MB/s]\n",
            "Downloading pytorch_model.bin:  43%|████▎     | 189M/436M [00:02<00:02, 84.6MB/s]\n",
            "Downloading pytorch_model.bin:  46%|████▌     | 199M/436M [00:02<00:02, 81.3MB/s]\n",
            "Downloading pytorch_model.bin:  48%|████▊     | 210M/436M [00:02<00:02, 84.4MB/s]\n",
            "Downloading pytorch_model.bin:  51%|█████     | 220M/436M [00:02<00:02, 83.4MB/s]\n",
            "Downloading pytorch_model.bin:  53%|█████▎    | 231M/436M [00:02<00:02, 86.4MB/s]\n",
            "Downloading pytorch_model.bin:  55%|█████▌    | 241M/436M [00:02<00:02, 88.9MB/s]\n",
            "Downloading pytorch_model.bin:  58%|█████▊    | 252M/436M [00:02<00:02, 90.9MB/s]\n",
            "Downloading pytorch_model.bin:  60%|██████    | 262M/436M [00:02<00:01, 91.6MB/s]\n",
            "Downloading pytorch_model.bin:  63%|██████▎   | 273M/436M [00:03<00:01, 90.9MB/s]\n",
            "Downloading pytorch_model.bin:  65%|██████▍   | 283M/436M [00:03<00:01, 90.8MB/s]\n",
            "Downloading pytorch_model.bin:  67%|██████▋   | 294M/436M [00:03<00:01, 91.6MB/s]\n",
            "Downloading pytorch_model.bin:  70%|██████▉   | 304M/436M [00:03<00:01, 92.1MB/s]\n",
            "Downloading pytorch_model.bin:  72%|███████▏  | 315M/436M [00:03<00:01, 91.9MB/s]\n",
            "Downloading pytorch_model.bin:  75%|███████▍  | 325M/436M [00:03<00:01, 91.0MB/s]\n",
            "Downloading pytorch_model.bin:  77%|███████▋  | 336M/436M [00:03<00:01, 89.7MB/s]\n",
            "Downloading pytorch_model.bin:  79%|███████▉  | 346M/436M [00:03<00:00, 90.2MB/s]\n",
            "Downloading pytorch_model.bin:  82%|████████▏ | 357M/436M [00:03<00:00, 92.1MB/s]\n",
            "Downloading pytorch_model.bin:  84%|████████▍ | 367M/436M [00:04<00:00, 93.5MB/s]\n",
            "Downloading pytorch_model.bin:  87%|████████▋ | 377M/436M [00:04<00:00, 93.5MB/s]\n",
            "Downloading pytorch_model.bin:  89%|████████▉ | 388M/436M [00:04<00:00, 92.9MB/s]\n",
            "Downloading pytorch_model.bin:  91%|█████████▏| 398M/436M [00:04<00:00, 81.5MB/s]\n",
            "Downloading pytorch_model.bin:  94%|█████████▍| 409M/436M [00:04<00:00, 83.7MB/s]\n",
            "Downloading pytorch_model.bin:  96%|█████████▌| 419M/436M [00:04<00:00, 85.6MB/s]\n",
            "Downloading pytorch_model.bin:  99%|█████████▊| 430M/436M [00:04<00:00, 80.6MB/s]\n",
            "Downloading pytorch_model.bin: 100%|██████████| 436M/436M [00:04<00:00, 88.2MB/s]\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "epoch 0: {'accuracy': 0.7745098039215687, 'f1': 0.8557993730407523}\n",
            "epoch 1: {'accuracy': 0.8406862745098039, 'f1': 0.8849557522123894}\n",
            "epoch 2: {'accuracy': 0.8553921568627451, 'f1': 0.8981001727115717}\n"
          ]
        }
      ],
      "source": [
        "launch_training_gpu(train_function_gpu, hps, train_args, stream_logs=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-kkLEusIptml"
      },
      "source": [
        "## Terminate Cluster"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Zcm1S1IWpxsH"
      },
      "source": [
        "Once you are done using the cluster, you can terminate it as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "-3xn723jps80",
        "outputId": "e6cae3c2-f113-4e12-f653-a57a0e700196"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">⠧</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Terminating </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">rh-v100</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[32m⠧\u001b[0m \u001b[1;36mTerminating \u001b[0m\u001b[1;32mrh-v100\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "gpu.teardown()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyBs5TaR1I_y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.2 (main, Feb 16 2023, 02:55:59) [Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "20e3d132b7ef4979916c341aea03dd2f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba7410d54dba475c9806a707b9be6d11": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_20e3d132b7ef4979916c341aea03dd2f",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Refreshing status for 1 cluster</span> <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">  0%</span> <span style=\"color: #008080; text-decoration-color: #008080\">-:--:--</span>\n</pre>\n",
                  "text/plain": "\u001b[1;36mRefreshing status for 1 cluster\u001b[0m \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
